{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d02a18-0225-437e-acdf-031ba7b9e8d2",
   "metadata": {},
   "source": [
    "***Tarea 3***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27651159-bd68-4766-93d4-e37079805637",
   "metadata": {},
   "source": [
    "Regresion Logistica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fd78162-5e30-4274-ac84-a5c7bbec6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 40.48%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from skimage.feature import hog\n",
    "\n",
    "\n",
    "\n",
    "# Ruta a la carpeta principal que contiene las subcarpetas\n",
    "ruta_carpeta_general = 'src/uco-animals-vs-plants-2025/train/'\n",
    "\n",
    "\n",
    "# Listas para almacenar las imágenes y etiquetas\n",
    "imagenes = []\n",
    "etiquetas = []\n",
    "\n",
    "# Función para extraer características HOG\n",
    "def extraer_caracteristicas_hog(imagen):\n",
    "    return hog(imagen, pixels_per_cell=(16, 16), cells_per_block=(2, 2), orientations=9)\n",
    "\n",
    "# Función para extraer características Fourier\n",
    "def extraer_caracteristicas_fourier(imagen):\n",
    "    f_transformada = np.fft.fft2(imagen)\n",
    "    f_shifted = np.fft.fftshift(f_transformada)  \n",
    "    magnitud_espectro = np.abs(f_shifted)        \n",
    "    # Redimensionamos para reducir tamaño y convertimos en vector\n",
    "    magnitud_espectro = cv2.resize(magnitud_espectro, (32, 32)).flatten()\n",
    "    return magnitud_espectro\n",
    "\n",
    "# Recorrer las subcarpetas\n",
    "for subcarpeta in os.listdir(ruta_carpeta_general):\n",
    "    ruta_subcarpeta = os.path.join(ruta_carpeta_general, subcarpeta)\n",
    "    if os.path.isdir(ruta_subcarpeta):\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            ruta_imagen = os.path.join(ruta_subcarpeta, archivo)\n",
    "            # Cargar la imagen en escala de grises y redimensionarla\n",
    "            imagen = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "            imagen = cv2.resize(imagen, (128, 128))\n",
    "            # Extraer características HOG y Fourier\n",
    "            caracteristicas_hog = extraer_caracteristicas_hog(imagen)\n",
    "            caracteristicas_fourier = extraer_caracteristicas_fourier(imagen)\n",
    "            # Combinar características de HOG y Fourier\n",
    "            caracteristicas_combinadas = np.concatenate([caracteristicas_hog, caracteristicas_fourier])\n",
    "            imagenes.append(caracteristicas_combinadas)\n",
    "            etiquetas.append(subcarpeta)\n",
    "\n",
    "# Convertir listas a arrays de NumPy\n",
    "X = np.array(imagenes)\n",
    "y = np.array(etiquetas)\n",
    "\n",
    "# Escalado de características antes de PCA\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar PCA para reducir la dimensionalidad\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de regresión logística\n",
    "modelo = LogisticRegression(max_iter=2000, solver='liblinear', C=0.1)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "precision = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {precision * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98926c09-d250-4539-9404-18ef86172616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta a la carpeta de prueba (test)\n",
    "ruta_carpeta_test = 'src/uco-animals-vs-plants-2025/test/'\n",
    "\n",
    "# Listas para almacenar las imágenes y nombres de archivo\n",
    "imagenes_test = []\n",
    "nombres_archivos = []\n",
    "\n",
    "# Función para extraer características HOG (misma que en el entrenamiento)\n",
    "def extraer_caracteristicas_hog(imagen):\n",
    "    return hog(imagen, pixels_per_cell=(16, 16), cells_per_block=(2, 2), orientations=9)\n",
    "\n",
    "# Función para extraer características Fourier (misma que en el entrenamiento)\n",
    "def extraer_caracteristicas_fourier(imagen):\n",
    "    f_transformada = np.fft.fft2(imagen)\n",
    "    f_shifted = np.fft.fftshift(f_transformada)\n",
    "    magnitud_espectro = np.abs(f_shifted)\n",
    "    magnitud_espectro = cv2.resize(magnitud_espectro, (32, 32)).flatten()\n",
    "    return magnitud_espectro\n",
    "\n",
    "# Recorrer la carpeta de prueba y ordenar los archivos alfabéticamente\n",
    "for archivo in sorted(os.listdir(ruta_carpeta_test)):\n",
    "    ruta_imagen = os.path.join(ruta_carpeta_test, archivo)\n",
    "    # Cargar la imagen en escala de grises y redimensionarla al mismo tamaño que las imágenes de entrenamiento\n",
    "    imagen = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "    if imagen is not None:\n",
    "        imagen = cv2.resize(imagen, (128, 128))  \n",
    "        # Extraer características HOG y Fourier\n",
    "        imagen_hog = extraer_caracteristicas_hog(imagen)\n",
    "        imagen_fourier = extraer_caracteristicas_fourier(imagen)\n",
    "        # Combinar características de HOG y Fourier\n",
    "        imagen_caracteristicas = np.concatenate([imagen_hog, imagen_fourier])\n",
    "        imagenes_test.append(imagen_caracteristicas)\n",
    "        nombres_archivos.append(archivo)\n",
    "\n",
    "# Convertir la lista de características combinadas a un array de NumPy\n",
    "X_test = np.array(imagenes_test)\n",
    "\n",
    "# Escalar las características usando el mismo escalador del entrenamiento\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reducir dimensiones con PCA\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Hacer predicciones con el modelo entrenado\n",
    "predicciones = modelo.predict(X_test_pca)\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'file': nombres_archivos,   \n",
    "    'label': predicciones       \n",
    "})\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "resultados.to_csv('Relog.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30826b09-26da-48b4-b76e-a888885ed7aa",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c84a1e4-0f5a-4045-bd27-c9092e58d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\migue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\migue\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 40ms/step - accuracy: 0.1102 - loss: 2.9077 - val_accuracy: 0.1128 - val_loss: 2.6962 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.2633 - loss: 2.2171 - val_accuracy: 0.1285 - val_loss: 3.0787 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 39ms/step - accuracy: 0.3553 - loss: 1.8551 - val_accuracy: 0.2643 - val_loss: 2.2274 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.3410 - loss: 1.8483 - val_accuracy: 0.3078 - val_loss: 1.9772 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 40ms/step - accuracy: 0.4155 - loss: 1.6438 - val_accuracy: 0.4307 - val_loss: 1.4755 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.4293 - loss: 1.6191 - val_accuracy: 0.4529 - val_loss: 1.5130 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.4485 - loss: 1.4810 - val_accuracy: 0.3041 - val_loss: 1.9330 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 41ms/step - accuracy: 0.4726 - loss: 1.4766 - val_accuracy: 0.2283 - val_loss: 3.2718 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.4781 - loss: 1.4744 - val_accuracy: 0.5573 - val_loss: 1.2959 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.5085 - loss: 1.3888 - val_accuracy: 0.4741 - val_loss: 1.2902 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.5141 - loss: 1.3345 - val_accuracy: 0.5092 - val_loss: 1.3442 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 42ms/step - accuracy: 0.5539 - loss: 1.2803 - val_accuracy: 0.1747 - val_loss: 2.8791 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.5605 - loss: 1.2576 - val_accuracy: 0.2680 - val_loss: 3.2581 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.5432 - loss: 1.2527 - val_accuracy: 0.3614 - val_loss: 2.1909 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 45ms/step - accuracy: 0.5799 - loss: 1.1977 - val_accuracy: 0.5157 - val_loss: 1.4427 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 46ms/step - accuracy: 0.5967 - loss: 1.1712 - val_accuracy: 0.6728 - val_loss: 0.8503 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.6086 - loss: 1.1127 - val_accuracy: 0.6201 - val_loss: 0.9687 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 47ms/step - accuracy: 0.6169 - loss: 1.0581 - val_accuracy: 0.5823 - val_loss: 1.1672 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - accuracy: 0.6393 - loss: 0.9966 - val_accuracy: 0.6802 - val_loss: 0.8240 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 48ms/step - accuracy: 0.6277 - loss: 1.0299 - val_accuracy: 0.3549 - val_loss: 2.0008 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 49ms/step - accuracy: 0.6328 - loss: 0.9885 - val_accuracy: 0.5952 - val_loss: 1.0848 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 50ms/step - accuracy: 0.6573 - loss: 0.9903 - val_accuracy: 0.4372 - val_loss: 1.4858 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 55ms/step - accuracy: 0.6624 - loss: 0.9662 - val_accuracy: 0.2957 - val_loss: 2.4149 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 55ms/step - accuracy: 0.6890 - loss: 0.8677 - val_accuracy: 0.6137 - val_loss: 1.2286 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.7015 - loss: 0.8588 - val_accuracy: 0.6673 - val_loss: 0.9664 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.6871 - loss: 0.8600 - val_accuracy: 0.7384 - val_loss: 0.6596 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.6912 - loss: 0.8559 - val_accuracy: 0.6793 - val_loss: 0.8722 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.7006 - loss: 0.8245 - val_accuracy: 0.7043 - val_loss: 0.7970 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.7119 - loss: 0.8092 - val_accuracy: 0.7135 - val_loss: 0.7746 - learning_rate: 2.5000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 55ms/step - accuracy: 0.6979 - loss: 0.8591 - val_accuracy: 0.7329 - val_loss: 0.6916 - learning_rate: 2.5000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.7183 - loss: 0.7754 - val_accuracy: 0.7061 - val_loss: 0.7662 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.7276 - loss: 0.7516 - val_accuracy: 0.7579 - val_loss: 0.6280 - learning_rate: 1.2500e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.7308 - loss: 0.7592 - val_accuracy: 0.6636 - val_loss: 0.9801 - learning_rate: 1.2500e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 62ms/step - accuracy: 0.7206 - loss: 0.7448 - val_accuracy: 0.7421 - val_loss: 0.6983 - learning_rate: 1.2500e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.7220 - loss: 0.7560 - val_accuracy: 0.7384 - val_loss: 0.6855 - learning_rate: 1.2500e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.7390 - loss: 0.7439 - val_accuracy: 0.7505 - val_loss: 0.6191 - learning_rate: 1.2500e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.7381 - loss: 0.7089 - val_accuracy: 0.7560 - val_loss: 0.6028 - learning_rate: 1.2500e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - accuracy: 0.7515 - loss: 0.6793 - val_accuracy: 0.7643 - val_loss: 0.6074 - learning_rate: 1.2500e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 55ms/step - accuracy: 0.7514 - loss: 0.6672 - val_accuracy: 0.7357 - val_loss: 0.6768 - learning_rate: 1.2500e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.7530 - loss: 0.6903 - val_accuracy: 0.7468 - val_loss: 0.6946 - learning_rate: 1.2500e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.7372 - loss: 0.7149 - val_accuracy: 0.7468 - val_loss: 0.6664 - learning_rate: 1.2500e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 57ms/step - accuracy: 0.7459 - loss: 0.7041 - val_accuracy: 0.7726 - val_loss: 0.5982 - learning_rate: 1.2500e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 63ms/step - accuracy: 0.7484 - loss: 0.6860 - val_accuracy: 0.7495 - val_loss: 0.6655 - learning_rate: 1.2500e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 53ms/step - accuracy: 0.7668 - loss: 0.6298 - val_accuracy: 0.7810 - val_loss: 0.5627 - learning_rate: 1.2500e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 58ms/step - accuracy: 0.7591 - loss: 0.6861 - val_accuracy: 0.7634 - val_loss: 0.5984 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.7566 - loss: 0.6729 - val_accuracy: 0.7708 - val_loss: 0.5834 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.7556 - loss: 0.6641 - val_accuracy: 0.7375 - val_loss: 0.6723 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.7576 - loss: 0.6682 - val_accuracy: 0.7246 - val_loss: 0.7517 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 54ms/step - accuracy: 0.7537 - loss: 0.6557 - val_accuracy: 0.7089 - val_loss: 0.7590 - learning_rate: 1.2500e-04\n",
      "Epoch 50/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.7595 - loss: 0.6835 - val_accuracy: 0.7542 - val_loss: 0.6630 - learning_rate: 6.2500e-05\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step \n",
      "Reporte de clasificación:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              Black-grass       0.56      0.08      0.15        59\n",
      "                 Charlock       0.74      0.91      0.81        87\n",
      "                 Cleavers       0.82      0.79      0.80        57\n",
      "         Common Chickweed       0.96      0.89      0.93       120\n",
      "             Common wheat       0.87      0.84      0.85        56\n",
      "                  Fat Hen       0.89      0.81      0.85        99\n",
      "         Loose Silky-bent       0.68      0.96      0.79       122\n",
      "                    Maize       0.91      0.95      0.93        41\n",
      "        Scentless Mayweed       0.87      0.93      0.90       113\n",
      "          Shepherds Purse       0.79      0.53      0.64        43\n",
      "Small-flowered Cranesbill       0.86      0.93      0.90        88\n",
      "               Sugar beet       0.85      0.84      0.84        79\n",
      "                      cat       0.64      0.51      0.57        35\n",
      "                      cow       0.37      0.88      0.52        26\n",
      "                      dog       0.29      0.20      0.24        30\n",
      "                    horse       0.60      0.11      0.19        27\n",
      "\n",
      "                 accuracy                           0.78      1082\n",
      "                macro avg       0.73      0.70      0.68      1082\n",
      "             weighted avg       0.78      0.78      0.76      1082\n",
      "\n",
      "Matriz de confusión:\n",
      " [[  5   0   0   0   1   1  50   0   0   0   0   2   0   0   0   0]\n",
      " [  0  79   5   0   1   0   0   0   1   0   1   0   0   0   0   0]\n",
      " [  0  11  45   0   0   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0 107   0   1   0   0   3   4   4   0   0   0   0   0]\n",
      " [  1   0   2   0  47   1   4   0   1   0   0   0   0   0   0   0]\n",
      " [  0   9   0   0   3  80   0   0   0   0   1   6   0   0   0   0]\n",
      " [  2   0   0   1   0   0 117   0   1   0   0   1   0   0   0   0]\n",
      " [  0   1   0   0   0   0   1  39   0   0   0   0   0   0   0   0]\n",
      " [  0   3   0   1   0   0   1   0 105   1   0   2   0   0   0   0]\n",
      " [  0   2   0   1   0   0   0   1   8  23   7   1   0   0   0   0]\n",
      " [  0   2   0   0   1   2   0   0   0   1  82   0   0   0   0   0]\n",
      " [  0   0   3   1   1   4   0   2   2   0   0  66   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   0   0  18   5  11   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2  23   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   5  17   6   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18   3   3]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuración inicial\n",
    "img_height, img_width = 32, 32  # Ajustar según tus necesidades\n",
    "ruta_carpeta_general = 'src/uco-animals-vs-plants-2025/train/'\n",
    "\n",
    "# Listas para almacenar imágenes y etiquetas\n",
    "imagenes = []\n",
    "etiquetas = []\n",
    "\n",
    "# Cargar imágenes y etiquetas desde las subcarpetas\n",
    "for subcarpeta in os.listdir(ruta_carpeta_general):\n",
    "    ruta_subcarpeta = os.path.join(ruta_carpeta_general, subcarpeta)\n",
    "    if os.path.isdir(ruta_subcarpeta):\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            ruta_imagen = os.path.join(ruta_subcarpeta, archivo)\n",
    "            imagen = cv2.imread(ruta_imagen)\n",
    "            imagen = cv2.resize(imagen, (img_height, img_width))\n",
    "            imagenes.append(imagen)\n",
    "            etiquetas.append(subcarpeta)\n",
    "\n",
    "# Convertir listas a arrays y normalizar\n",
    "X = np.array(imagenes) / 255.0  # Normalizar entre 0 y 1\n",
    "y = np.array(etiquetas)\n",
    "\n",
    "# Binarización de las etiquetas\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "num_classes = len(lb.classes_)\n",
    "\n",
    "# División de datos en entrenamiento y prueba\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aumento de datos\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Definir el modelo Sequential con capas convolucionales\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), padding='same', input_shape=(img_height, img_width, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Conv2D(32, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks para evitar sobreajuste y reducir la tasa de aprendizaje\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluación del modelo en el conjunto de prueba\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Imprimir la matriz de confusión y el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_true_classes, y_pred_classes, target_names=lb.classes_))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_true_classes, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905d0bd9-93b4-4753-9c96-0f240b9317f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de la carpeta de prueba\n",
    "ruta_carpeta_test = 'src/uco-animals-vs-plants-2025/test/'\n",
    "\n",
    "# Listas para almacenar nombres de archivos y predicciones\n",
    "nombres_archivos = []\n",
    "predicciones = []\n",
    "\n",
    "img_height, img_width = 32, 32\n",
    "\n",
    "# Cargar imágenes desde la carpeta de prueba\n",
    "for archivo in os.listdir(ruta_carpeta_test):\n",
    "    ruta_imagen = os.path.join(ruta_carpeta_test, archivo)\n",
    "    imagen = cv2.imread(ruta_imagen)\n",
    "    if imagen is not None:\n",
    "        imagen = cv2.resize(imagen, (img_height, img_width))\n",
    "        imagen = np.array(imagen) / 255.0  # Normalizar la imagen\n",
    "        imagen = np.expand_dims(imagen, axis=0)  # Añadir una dimensión para el batch\n",
    "\n",
    "        # Realizar la predicción\n",
    "        prediccion = model.predict(imagen, verbose=0)\n",
    "        clase_predicha = np.argmax(prediccion, axis=1)[0]  # Obtener la clase predicha\n",
    "        predicciones.append(clase_predicha)  # Almacenar la predicción\n",
    "        nombres_archivos.append(archivo)  # Almacenar el nombre del archivo\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'file': nombres_archivos,   # Nombre de archivo de la imagen\n",
    "    'label': lb.classes_[predicciones]  # Etiqueta predicha\n",
    "})\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "resultados.to_csv('CNN.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002d73fc-215e-44b7-acfe-15c40620bd65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (TensorFlow)",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
